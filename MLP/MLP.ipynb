{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORT THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mention watch 1 oz episode youll...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres family little boy jake think ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stun f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewers mention watch 1 oz episode youll...  positive\n",
       "1  wonderful little production film technique una...  positive\n",
       "2  think wonderful way spend time hot summer week...  positive\n",
       "3  basically theres family little boy jake think ...  negative\n",
       "4  petter matteis love time money visually stun f...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words =stopwords.words('english')\n",
    "def clean_data(text):\n",
    "    text = re.sub(r'<br />', ' ', text) #Removes Html tag\n",
    "    text = re.sub(r'[^\\ a-zA-Z0-9]+', '', text)  #Removes non alphanumeric\n",
    "    text = re.sub(r'^\\s*|\\s\\s*', ' ', text).strip() #Removes extra whitespace, tabs\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.lower().split() #Converts text to lowercase\n",
    "    cleaned_text = list()\n",
    "    for word in text:        \n",
    "        if word in stop_words:    #Removes Stopwords, i.e words that don't convey any meaningful context/sentiments\n",
    "            continue    \n",
    "        word = lemmatizer.lemmatize(word, pos = 'v')    #Lemmatize words, pos = verbs, i.e playing, played becomes play\n",
    "        cleaned_text.append(word)\n",
    "    text = ' '.join(cleaned_text)\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: clean_data(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one reviewers mention watch 1 oz episode youll...\n",
       "1        wonderful little production film technique una...\n",
       "2        think wonderful way spend time hot summer week...\n",
       "3        basically theres family little boy jake think ...\n",
       "4        petter matteis love time money visually stun f...\n",
       "                               ...                        \n",
       "49995    think movie right good job wasnt creative orig...\n",
       "49996    bad plot bad dialogue bad act idiotic direct a...\n",
       "49997    catholic teach parochial elementary school nun...\n",
       "49998    im go disagree previous comment side maltin on...\n",
       "49999    one expect star trek movies high art fan expec...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERTING CATEGORICAL FEATURES TO LABELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "Y_train_label = le.fit_transform(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ONE-LAYER MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZING THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERTING TEXT TO SEQUENCES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153603"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPLY PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in df['review']])\n",
    "X_new= pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1437"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1437)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y_train_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1437)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERT ARRAYS TO TENSORS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "test_data=TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "\n",
    "#dataloader\n",
    "batch_size=64\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD PRE-TRAINED WORD2VEC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pretrained word2vec\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_values=np.zeros((vocab_size,300))\n",
    "for key,value in tokenizer.word_index.items():\n",
    "    if key in wv:\n",
    "        pretrained_values[value,:]=wv[key]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ONE-LAYER MLP WITH AN EMBEDDING LAYER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_ONE(nn.Module):\n",
    "    def __init__(self, pretrained_values,input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.embedding.load_state_dict({'weight': pretrained_values})\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        self.fc = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embeds=embedded.sum(dim=1)\n",
    "        out=self.fc(embeds)\n",
    "        out=torch.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_values=torch.Tensor(pretrained_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIMENSIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = vocab_size\n",
    "n_embed = 300\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "net = MLP_ONE(pretrained_values,n_vocab, n_embed, n_hidden, n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZER, LOSS FUNCTION AND LEARNING RATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACCURACY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds) # 0.75 --> 1 0.4 --> 0\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #Train mode is on\n",
    "    for batchX,batchy in iterator:\n",
    "        batchX.requires_grad = True\n",
    "        optimizer.zero_grad() #Reset the gradients\n",
    "        predictions= model(batchX.long()).squeeze(1) ## forward propagation\n",
    "        loss = criterion(predictions, batchy)\n",
    "        loss.backward() ## backward propagation / calculate gradients\n",
    "        optimizer.step() ## update parameters\n",
    "        acc = binary_accuracy(predictions, batchy)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()   \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "#train1(net, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate1(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() #Evaluation mode is on\n",
    "    with torch.no_grad():\n",
    "        for batchX,batchy in iterator:\n",
    "            predictions = model(batchX.long()).squeeze(1) \n",
    "            loss = criterion(predictions, batchy)\n",
    "            acc = binary_accuracy(predictions, batchy)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "#evaluate1(net, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING THE MODEL FOR 5 EPOCHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 01 | Epoch Time: 516.22\n",
      "\tTrain Loss: 0.402 | Train Acc: 84.27%\n",
      "\tTest Loss: 0.255 |  Test. Acc: 90.15%\n",
      "\n",
      "\n",
      "\tEpoch: 02 | Epoch Time: 582.42\n",
      "\tTrain Loss: 0.184 | Train Acc: 92.81%\n",
      "\tTest Loss: 0.283 |  Test. Acc: 88.88%\n",
      "\n",
      "\n",
      "\tEpoch: 03 | Epoch Time: 581.82\n",
      "\tTrain Loss: 0.126 | Train Acc: 95.09%\n",
      "\tTest Loss: 0.314 |  Test. Acc: 89.62%\n",
      "\n",
      "\n",
      "\tEpoch: 04 | Epoch Time: 616.86\n",
      "\tTrain Loss: 0.075 | Train Acc: 97.21%\n",
      "\tTest Loss: 0.449 |  Test. Acc: 87.18%\n",
      "\n",
      "\n",
      "\tEpoch: 05 | Epoch Time: 654.10\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.41%\n",
      "\tTest Loss: 0.550 |  Test. Acc: 87.59%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "import time\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train1(net, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate1(net, test_loader, criterion)\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    \n",
    "    print(f'\\tEpoch: {epoch+1:02} | Epoch Time: {end_time-start_time:.2f}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TWO-LAYER MLP USING TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERTING WORDS TO VECTORS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=100) #Create a tf-idf vectorizer\n",
    "vectorizer.fit(df['review']) #Fit the vectorizer on the data by mapping the words onto the index values. \n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(vectorizer.idf_)\n",
    "\n",
    "X = vectorizer.transform(df['review']) #Build a tf-idf based feature vector for the first document (the first sentence).\n",
    "#print(type(vector))\n",
    "#print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERTING CATEGORICAL FEATURES TO LABELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mention watch 1 oz episode youll...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres family little boy jake think ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stun f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewers mention watch 1 oz episode youll...  positive\n",
       "1  wonderful little production film technique una...  positive\n",
       "2  think wonderful way spend time hot summer week...  positive\n",
       "3  basically theres family little boy jake think ...  negative\n",
       "4  petter matteis love time money visually stun f...  positive"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le2 = preprocessing.LabelEncoder()\n",
    "Y_train_label2 = le.fit_transform(df['sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLIT DATA RANDOMLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_train_label2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 20039, 1: 19961})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERT DATA TO TENSORS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(X_train.toarray()), torch.FloatTensor(y_train))\n",
    "test_data=TensorDataset(torch.FloatTensor(X_test.toarray()), torch.FloatTensor(y_test))\n",
    "\n",
    "#dataloader\n",
    "batch_size=64\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt calls the __iter__() method on the iterable, and then calls __next__() on the returned iterator until it reaches \\nthe end of the iterator\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=iter(train_loader).next() \n",
    "\"\"\"\n",
    "It calls the __iter__() method on the iterable, and then calls __next__() on the returned iterator until it reaches \n",
    "the end of the iterator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TWO-LAYER MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron2(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size):\n",
    "    # Call initializer function of the super class \n",
    "        super(MultilayerPerceptron2, self).__init__()\n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        #whenever this model is called, those layers in the sequential block \n",
    "        #will be processed in the order given to the block. \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size,self.hidden_size ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        output = self.model(x) #call the model defined above for forward propagation. \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron2(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5283, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size=X_train.shape[1]\n",
    "hidden_size=100  ## common values: multiple of a power of 2 \n",
    "net2 = MultilayerPerceptron2(input_size,hidden_size)\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZER, LOSS FUNCTION, LEARNING RATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(net.parameters(), lr=1e-3)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "criterion2 = nn.BCELoss()\n",
    "optimizer2 = torch.optim.Adam(net2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACCURACY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy2(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds) # 0.75 --> 1 0.4 --> 0\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36765016741752626, 0.8633)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train2(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #Train mode is on\n",
    "    for batchX,batchy in iterator:\n",
    "        batchX.requires_grad = True\n",
    "        optimizer.zero_grad() #Reset the gradients\n",
    "        predictions= model(batchX).squeeze(1) ## forward propagation\n",
    "        loss = criterion2(predictions, batchy)\n",
    "        loss.backward() ## backward propagation / calculate gradients\n",
    "        optimizer.step() ## update parameters\n",
    "        \n",
    "        acc = binary_accuracy2(predictions, batchy)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "train2(net2, train_loader, optimizer2, criterion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2600648891014658, 0.8908240445859873)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate2(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() #Evaluation mode is on\n",
    "    with torch.no_grad():\n",
    "        for batchX,batchy in iterator:\n",
    "            predictions = model(batchX).squeeze(1) \n",
    "            loss = criterion2(predictions, batchy)\n",
    "            acc = binary_accuracy2(predictions, batchy)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "evaluate2(net2, test_loader, criterion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN THE MODEL FOR 5 EPOCHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 01 | Epoch Time: 7.86\n",
      "\tTrain Loss: 0.236 | Train Acc: 90.70%\n",
      "\tTest Loss: 0.256 |  Test. Acc: 89.02%\n",
      "\n",
      "\n",
      "\tEpoch: 02 | Epoch Time: 6.91\n",
      "\tTrain Loss: 0.210 | Train Acc: 91.76%\n",
      "\tTest Loss: 0.262 |  Test. Acc: 88.95%\n",
      "\n",
      "\n",
      "\tEpoch: 03 | Epoch Time: 6.92\n",
      "\tTrain Loss: 0.193 | Train Acc: 92.47%\n",
      "\tTest Loss: 0.271 |  Test. Acc: 88.62%\n",
      "\n",
      "\n",
      "\tEpoch: 04 | Epoch Time: 6.39\n",
      "\tTrain Loss: 0.177 | Train Acc: 93.19%\n",
      "\tTest Loss: 0.282 |  Test. Acc: 88.33%\n",
      "\n",
      "\n",
      "\tEpoch: 05 | Epoch Time: 6.46\n",
      "\tTrain Loss: 0.161 | Train Acc: 93.77%\n",
      "\tTest Loss: 0.292 |  Test. Acc: 88.33%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train2(net2, train_loader, optimizer2, criterion2)\n",
    "    test_loss, test_acc = evaluate2(net2, test_loader, criterion2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    \n",
    "    ##if valid_loss < best_valid_loss:\n",
    "    #    best_valid_loss = valid_loss\n",
    "    #    torch.save(model.state_dict(), 'tut1-model.pt')  ##\n",
    "    \n",
    "    print(f'\\tEpoch: {epoch+1:02} | Epoch Time: {end_time-start_time:.2f}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train Accuracy: ')\n",
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8543"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test Accuracy: ')\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LOGISTIC REGRESSION CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.913825"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train Accuracy: ')\n",
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.891"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test Accuracy: ')\n",
    "clf.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
